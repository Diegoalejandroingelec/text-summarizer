{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv8Y8QXY7d5l"
   },
   "source": [
    "# TEXT SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NW3TMk27d5m"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIHpn5oX7d5n"
   },
   "source": [
    "Customer reviews can be lengthy and detailed. Manually analysing these reviews, as you might guess, takes a long time. This is where Natural Language Processing's application can be put to use to develop a short summary for lengthy reviews.\n",
    "\n",
    "Our objective here is to generate a summary for the **\"Amazon Fine Food reviews\"** using the **abstraction-based** and as well as **extraction-based** text summarization approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WED27lnA7d5n"
   },
   "source": [
    "# Project pipeline\n",
    "\n",
    "1. Understanding Text Summarization\n",
    "2. Text pre-processing\n",
    "3. Abstractive Text Summarization using LSTM, ENCODER-DECODER architecture\n",
    "4. Extractive Text Summarization using Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLslFszL7d5o"
   },
   "source": [
    "## 1. Understanding Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIW6QXMV7d5o"
   },
   "source": [
    "**Text summarization** is a Natural Language Processing application which produces short and meaningful summary of a lengthy paragraph thereby helping us to understand the essence of the topic in an efficient way.\n",
    "\n",
    "**Types of Text Summarization**\n",
    "\n",
    "1. Abstrative Based\n",
    "2. Extractive Based\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0DQxiqM7d5o"
   },
   "source": [
    "In Abstractive based, we generate new sentences from the original text. The sentences generated through abstractive summarization might not be present in the original text.\n",
    "\n",
    "In Extractive based, we identify the important sentences or phrases from the original text and extract only those from the text. Those extracted sentences would be our summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q9Qpyqk7d5o"
   },
   "source": [
    "Custom Attention Layer: Keras does not officially support attention layer. We will use a third-party implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUValOzcHtEK"
   },
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Jpu8qLEFxcY",
    "outputId": "c4b3a9f3-18d3-4ee2-ffa8-b1c5b75c7917"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 14:19:21.656556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 14:19:21.783907: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-04-22 14:19:21.784030: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-04-22 14:19:21.784219: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-22 14:19:21.830721: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy) (65.7.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (937 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m937.8/937.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m952.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/site-packages (from spacy) (1.23.5)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.18.1\n",
      "  Using cached pydantic_core-2.18.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m867.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, cloudpathlib, pydantic, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.0 pydantic-core-2.18.1 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.11.0 wasabi-1.1.2 weasel-0.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.9/site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (65.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVakjZ3oICgx"
   },
   "source": [
    "## Read the dataset\n",
    "\n",
    "This dataset consists of reviews of fine foods from Amazon. The data spans a period of more than 10 years, including all ~500,000 reviews up to October 2012. These reviews include product and user information, ratings, plain text review, and summary. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "We’ll take a sample of 50,000 reviews to reduce the training time of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yTSyKdF8I4V",
    "outputId": "34d6e2ce-8b8d-4b7f-b8cc-e9ae4e4a1e99"
   },
   "outputs": [],
   "source": [
    "# # mount the dataset in google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plxAO_Xf8K5b",
    "outputId": "bbf90197-d379-440d-8ff6-64c68a9b24a3"
   },
   "outputs": [],
   "source": [
    "# cd /content/gdrive/My Drive/Colab Notebooks/NLP Assignment 3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rCkwTv7x8rlA"
   },
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wnK5o4Z1Fxcj"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGNQKvCaISIn"
   },
   "source": [
    "## Drop Duplicates and NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cjul88oOFxcr"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0xLYACiFxdJ"
   },
   "source": [
    "## 2. Text Pre-processing\n",
    "\n",
    "Before we start developing the model, we must first complete some basic preprocessing tasks. Using messy and sloppy text data can be devastating. As a result, in this stage, we will remove all unneeded symbols, characters, and other elements from the text that do not affect the problem's goal.\n",
    "\n",
    "Here is the dictionary that we will use for expanding the contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0s6IY-x2FxdL"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JFRXFHmI7Mj"
   },
   "source": [
    "We will perform the below pre-processing tasks for our data:\n",
    "\n",
    "1.Convert everything to lowercase\n",
    "\n",
    "2.Remove HTML tags\n",
    "\n",
    "3.Contraction mapping\n",
    "\n",
    "4.Remove (‘s)\n",
    "\n",
    "5.Remove any text inside the parenthesis ( )\n",
    "\n",
    "6.Eliminate punctuations and special characters\n",
    "\n",
    "7.Remove stopwords\n",
    "\n",
    "8.Remove single characters\n",
    "\n",
    "**Let’s define the function for performing the above pre-processing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kpwT75RN7d5r"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    # lower\n",
    "    newString = text.lower()\n",
    "    # remove HTML\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    # Remove any text inside the parenthesis\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    # remove double quotes\n",
    "    newString = re.sub('\"','', newString)\n",
    "    # contraction mapping\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])\n",
    "    # remove 's\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    # Eliminate punctuations and special characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    # Remove stopwords\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    # Remove short words\n",
    "    for i in tokens:\n",
    "        if len(i)>1:\n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A2QAeCHWFxdY"
   },
   "outputs": [],
   "source": [
    "# Cleaning the \"Text\" Column\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snRZY8wjLao2"
   },
   "source": [
    "Let us look at the first 2 preprocessed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCAIkhWbFxdh",
    "outputId": "dceb0a52-3334-4dee-b4de-73d639a670be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GsRXocxoFxd-"
   },
   "outputs": [],
   "source": [
    "# Cleaning the \"Summary\" Column\n",
    "\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZeD0gs6Lnb-"
   },
   "source": [
    "Let us look at the first 2 preprocessed summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQJdZcAzFxee",
    "outputId": "195aec5d-d7e6-4906-d34f-866d6fae7a4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good quality dog food', 'not as advertised']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_summary[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L1zLpnqsFxey"
   },
   "outputs": [],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT_D2cLiLy77"
   },
   "source": [
    "## Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sYK390unFxfA"
   },
   "outputs": [],
   "source": [
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm8Fk2TCL7Sp"
   },
   "source": [
    "## Understanding the distribution of the sequences\n",
    "\n",
    "Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MdF76AHHFxgw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "# length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "# length_df.hist(bins = 30)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwdSGIhGMEbz"
   },
   "source": [
    "**NOTE: We can fix the maximum length of the summary to 8 since that seems to be the majority summary length.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w0WhTi87d5u"
   },
   "source": [
    "Let us understand the proportion of the length of summaries below 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JRjwdIOFxg3",
    "outputId": "cd21f1ac-b6a9-4a18-e78c-1216a2a86886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458130834941876\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split()) <= 8):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYB4Ga9KMjEu"
   },
   "source": [
    "**We observe that 94% of the summaries have length below 8. So, we can fix maximum length of summary to 8.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndoW6oXH7d5u",
    "outputId": "1f6ee3ab-e2bb-413f-e19a-20deb977dc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9130318682240923\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_text']:\n",
    "    if(len(i.split()) <= 80):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Bpwx1RD7d5u"
   },
   "source": [
    "Let us fix the maximum length of review to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZKD5VOWqFxhC"
   },
   "outputs": [],
   "source": [
    "max_text_len=80\n",
    "max_summary_len=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6d48E-8M4VO"
   },
   "source": [
    "Let us select the reviews and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yY0tEJP0FxhI"
   },
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "\n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary}) # new dataframe to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "N2T2aL6e7d5v"
   },
   "outputs": [],
   "source": [
    "# add the START and END special tokens at the beginning and end of the summary. Here, We have chosen sostok and eostok as START and END tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "xJfG0XZz7d5y",
    "outputId": "ad886a6e-0898-46bd-e5ef-17fcb4adcd8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better</td>\n",
       "      <td>sostok good quality dog food eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo</td>\n",
       "      <td>sostok not as advertised eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat famil...</td>\n",
       "      <td>sostok delight says it all eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal</td>\n",
       "      <td>sostok cough medicine eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy taffy delivery quick taffy lover deal</td>\n",
       "      <td>sostok great taffy eostok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "0                                     bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better   \n",
       "1                                                                    product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo   \n",
       "2  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat famil...   \n",
       "3                                                                              looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal   \n",
       "4                                                                                                                      great taffy great price wide assortment yummy taffy delivery quick taffy lover deal   \n",
       "\n",
       "                               summary  \n",
       "0  sostok good quality dog food eostok  \n",
       "1      sostok not as advertised eostok  \n",
       "2    sostok delight says it all eostok  \n",
       "3         sostok cough medicine eostok  \n",
       "4            sostok great taffy eostok  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9jIYHm07d5y"
   },
   "source": [
    "## Train-Test Split and Prepare the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RakakKHcFxhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']), np.array(df['summary']),\n",
    "                                       test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# A tokenizer builds the vocabulary and converts a word sequence to an integer sequence.\n",
    "# We will now build tokenizers for text and summary.\n",
    "\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzvLwYL_PDcx"
   },
   "source": [
    "## Rarewords and its Coverage on Reviews column\n",
    "\n",
    "The threshold is taken as 4 which means word whose count is below 4 is considered as a **rare word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8KronV2Fxhx",
    "outputId": "6a803b93-5189-4ced-d805-30b357c2a607",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 67.49183122602642\n",
      "Total Coverage of rare words: 0.9498986005845663\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "\n",
    "print(\"% of rare words in vocabulary:\", (cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\", (freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBaVkJry7d5z",
    "outputId": "613f3bb4-48d5-4a7f-e786-aa4ba6ba4811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57009\n",
      "84468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cnt),print(tot_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N6voWJHCNP3",
    "outputId": "3071323a-1fa0-4208-d953-75797a731753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27459\n"
     ]
    }
   ],
   "source": [
    "print(tot_cnt-cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geDn_Iy_7d5z",
    "outputId": "83caf242-0956-4b55-85e2-30837c793b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76175\n",
      "8019277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(freq),print(tot_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So-J-5kzQIeO"
   },
   "source": [
    "NOTE:\n",
    "\n",
    "* **tot_cnt** gives the size of vocabulary (which means every unique words in the text)\n",
    "\n",
    "*   **cnt** gives me the no. of rare words whose count falls below threshold\n",
    "\n",
    "*  **tot_cnt - cnt** gives me the top most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr4cqCSM7d51"
   },
   "source": [
    "Let us define the tokenizer with **top most common words** for reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ktojgap7d51"
   },
   "source": [
    "## Reviews Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "J2giEsF3Fxh3"
   },
   "outputs": [],
   "source": [
    "# prepare a tokenizer for reviews on training data\n",
    "\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) # num_words: the maximum number of words to keep, based on word frequency.\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr)\n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCbGMsm4FxiA",
    "outputId": "658be46d-37a2-47b3-fcc6-ba2c8b5d3730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27460"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvVLEmeTBSpl",
    "outputId": "586e7289-9d23-4eb4-a308-3ee0c17a214e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 120,  151,   44, ...,    0,    0,    0],\n",
       "       [   1, 2926, 1122, ...,    0,    0,    0],\n",
       "       [ 196,  122,   15, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  26,   89, 1126, ...,    0,    0,    0],\n",
       "       [  10,  182,   49, ...,    0,    0,    0],\n",
       "       [ 955,  404,   15, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQfKP3sqRxi9"
   },
   "source": [
    "## Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eRHqyBkBFxiJ"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KInA6O6ZSkJz"
   },
   "source": [
    "## Rarewords and its Coverage on the summary column\n",
    "\n",
    "The threshold is taken as 6 which means word whose count is below 6 is considered as a **rare word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzE5OiRLFxiM",
    "outputId": "b0fd34b4-f401-4204-d52a-3a94de20e7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 75.03878374185543\n",
      "Total Coverage of rare words: 2.1217508472879163\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "\n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piRYVWwN7d52",
    "outputId": "84dba26c-2a7d-429e-fecb-bc74ff094681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19348\n",
      "25784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cnt),print(tot_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qf2LWSfSDb_1",
    "outputId": "4d961734-8e78-47ca-8222-e514f38c553b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6436\n"
     ]
    }
   ],
   "source": [
    "print(tot_cnt - cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN_aaMJE7d53",
    "outputId": "c9ec21bb-e3a0-4d08-b9d5-a0199e7a2281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31997\n",
      "1508047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(freq),print(tot_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PBhzKuRSw_9"
   },
   "source": [
    "Let us define the tokenizer with **top most common words for summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-fswLvIgFxiR"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr)\n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ernIb4cj7d53"
   },
   "outputs": [],
   "source": [
    "#deleting the rows that contain only START and END tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qEgMzhru7d53"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tR1JT5Sd7d54"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtlDcthFxip"
   },
   "source": [
    "# Abstractive Text Summarization - Model building\n",
    "\n",
    "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.\n",
    "\n",
    "**Return Sequences = True**: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
    "\n",
    "**Return State = True**: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
    "\n",
    "**Initial State**: This is used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "**Stacked LSTM**: Stacked LSTM has multiple layers of LSTM stacked on top of each other.\n",
    "This leads to a better representation of the sequence.\n",
    "\n",
    "Here, we are building a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXef38nBFxir",
    "outputId": "397494fe-9f23-4e96-d190-d23bb60076aa"
   },
   "outputs": [],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# # Encoder\n",
    "# encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "# #embedding layer\n",
    "# enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "# #encoder lstm 1\n",
    "# encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# #encoder lstm 2\n",
    "# encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# #encoder lstm 3\n",
    "# encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "# decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# #embedding layer\n",
    "# dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "# dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "# decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# # Attention layer\n",
    "# attn_layer = AttentionLayer(name='attention_layer')\n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# # Concat attention input and decoder LSTM output\n",
    "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# #dense layer\n",
    "# decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "# decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# # Define the model\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "WLJ5vlSLYZv7"
   },
   "outputs": [],
   "source": [
    "# model=tf.keras.models.load_model(\"/tmp/weights.10-0.60.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZVlfRuMUcoP"
   },
   "source": [
    "Sparse categorical cross-entropy as the loss function since it converts the integer sequence to a one-hot vector on the fly. This overcomes any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Lwfi1Fm8Fxiz"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "VI2cDnUpYFYc"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0ykDbxfUhyw"
   },
   "source": [
    "EarlyStopping monitors the validation loss (val_loss). Our model will stop training once the validation loss increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "s-A3J92MUljB"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "wMBULQ5zYFYc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "checkpoint=tf.keras.callbacks.ModelCheckpoint(\"./weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw6CVECaUq5b"
   },
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETnPzA4OFxi3",
    "outputId": "6ddba984-0e87-4dd5-a84b-2fad802ad925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 15:45:42.792660: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x556f722265c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-17 15:45:42.792695: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-04-17 15:45:42.797283: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-17 15:45:42.988488: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2121/2121 [==============================] - 1777s 832ms/step - loss: 3.1450 - val_loss: 2.8957\n",
      "Epoch 2/50\n",
      "2121/2121 [==============================] - 1699s 801ms/step - loss: 2.7837 - val_loss: 2.6944\n",
      "Epoch 3/50\n",
      "2121/2121 [==============================] - 1681s 792ms/step - loss: 2.6206 - val_loss: 2.5734\n",
      "Epoch 4/50\n",
      "2121/2121 [==============================] - 1668s 786ms/step - loss: 2.5097 - val_loss: 2.4818\n",
      "Epoch 5/50\n",
      "2121/2121 [==============================] - 1701s 802ms/step - loss: 2.4240 - val_loss: 2.4099\n",
      "Epoch 6/50\n",
      "2121/2121 [==============================] - 1705s 804ms/step - loss: 2.3575 - val_loss: 2.3745\n",
      "Epoch 7/50\n",
      "2121/2121 [==============================] - 1714s 808ms/step - loss: 2.3039 - val_loss: 2.3224\n",
      "Epoch 8/50\n",
      "2121/2121 [==============================] - 1674s 789ms/step - loss: 2.2589 - val_loss: 2.2917\n",
      "Epoch 9/50\n",
      "2121/2121 [==============================] - 1783s 841ms/step - loss: 1.9638 - val_loss: 2.1444\n",
      "Epoch 21/50\n",
      "2121/2121 [==============================] - 1751s 825ms/step - loss: 1.9481 - val_loss: 2.1408\n",
      "Epoch 22/50\n",
      "2121/2121 [==============================] - 1739s 820ms/step - loss: 1.9340 - val_loss: 2.1415\n",
      "Epoch 23/50\n",
      "2121/2121 [==============================] - 1718s 810ms/step - loss: 1.9202 - val_loss: 2.1357\n",
      "Epoch 24/50\n",
      "2121/2121 [==============================] - 1713s 808ms/step - loss: 1.9065 - val_loss: 2.1338\n",
      "Epoch 25/50\n",
      "2121/2121 [==============================] - 1725s 813ms/step - loss: 1.8942 - val_loss: 2.1365\n",
      "Epoch 26/50\n",
      "2121/2121 [==============================] - 1719s 810ms/step - loss: 1.8815 - val_loss: 2.1293\n",
      "Epoch 27/50\n",
      "2121/2121 [==============================] - 1738s 820ms/step - loss: 1.8699 - val_loss: 2.1308\n",
      "Epoch 28/50\n",
      "2121/2121 [==============================] - 1717s 810ms/step - loss: 1.8579 - val_loss: 2.1295\n",
      "Epoch 28: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr, y_tr[:,:-1]],\n",
    "                  y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:],\n",
    "                  epochs=50,\n",
    "                  callbacks=[es, checkpoint],\n",
    "                  batch_size=128,\n",
    "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:])\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nvli1uVOYFYd",
    "outputId": "973507ef-2b54-4d7b-ed49-4d229a012559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezKYOp2UxG5"
   },
   "source": [
    "## Understanding the Diagnostic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "tDTNLAURFxjE",
    "outputId": "36f852ae-e1ff-4ffb-c063-12eea245fb71"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSyx-HvpUz2o"
   },
   "source": [
    "Next, let’s build the dictionary to convert the index to word for target and source vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "sBX0zZnOFxjW"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM_nU_VvFxjq",
    "tags": []
   },
   "source": [
    "## Inference\n",
    "\n",
    "Set up the inference for the encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 14:24:08.655127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:08.735714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:08.737589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:08.739843: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 14:24:08.740192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:08.742046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:08.743855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:09.794329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:09.796420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:09.798296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-22 14:24:09.800074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13653 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from attention import AttentionLayer\n",
    "model=tf.keras.models.load_model(\"./weights.28-2.13.hdf5\", custom_objects={\"AttentionLayer\": AttentionLayer}) # Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 80, 100)      2746000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 80, 300),    481200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 80, 300),    721200      ['lstm[0][0]']                   \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    643700      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 80, 300),    721200      ['lstm_1[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 300),  481200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 300),  180300     ['lstm_2[0][0]',                 \n",
      " r)                              (None, None, 80))                'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 600)    0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 6437)  3868637     ['concat_layer[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,843,437\n",
      "Trainable params: 9,843,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the variables/layers directly from the loaded model\n",
    "encoder_inputs = model.get_layer('input_1').output\n",
    "encoder_outputs = model.get_layer('lstm_2').output[0]  # Access the first output which is the output of the last LSTM layer in the encoder\n",
    "state_h = model.get_layer('lstm_2').output[1]  # Access the hidden state\n",
    "state_c = model.get_layer('lstm_2').output[2]  # Access the cell state\n",
    "\n",
    "decoder_inputs = model.get_layer('input_2').output\n",
    "dec_emb_layer = model.get_layer('embedding_1')\n",
    "decoder_lstm = model.get_layer('lstm_3')\n",
    "attn_layer = model.get_layer('attention_layer')\n",
    "decoder_dense = model.get_layer('time_distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9QkrNV-4Fxjt"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOiyk4ToWe74"
   },
   "source": [
    "We are defining a function below which is the implementation of the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "6f6TTFnBFxj6"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GuDf4TPWt6_"
   },
   "source": [
    "Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "aAUntznIFxj9"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gM4ALyfWwA9"
   },
   "source": [
    "Here are a few summaries generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUtQmQTmFxkI",
    "outputId": "ebfe060b-6eac-47e1-facd-ca0b3e7cb305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: happy gluten free puffs thought kiddo would miss finger food together allergies loves \n",
      "Original summary: thrilled it is gluten free \n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted summary:  great gluten free snack\n",
      "\n",
      "\n",
      "Review: like goji berries purchased products navitas naturals good quality received immediately opened tried smelled tasted slightly moldy see mold opened second package tasted put away disregarded taste started feel little sick went back packages cut perfectly fine looking berries open filled mold definitely purchasing product matter packed nutrients made temporarily paranoid dried stuff eat looks fine outside shame returns item noticing reviews mold problem fairly common \n",
      "Original summary: mold \n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted summary:  mold mold\n",
      "\n",
      "\n",
      "Review: kids thought much like pasta tomato soup crazy \n",
      "Original summary: just okay \n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted summary:  kids love it\n",
      "\n",
      "\n",
      "Review: coffee start every morning weak strong \n",
      "Original summary: great coffee \n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: bought item gift health conscious mother picky oils little worried decided buy anyway well really likes uses part homemade salad dressing told bottles lasted many months hemp oil like others little bit goes long way wish though came glass versus plastic ones thrown away \n",
      "Original summary: my mother really likes this oil \n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted summary:  great for cooking\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRHK89EFbbva",
    "outputId": "4621c7a4-6ebc-4307-aebc-f38c3d1a7a52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "5pqK9M7KS_vS"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_number = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epcPL_6qauV8",
    "outputId": "587187a5-0b92-4470-8a2a-0943bd0da343",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenized_references = [[nltk.word_tokenize(seq2summary(refs))] for refs in y_val[:evaluation_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenized_candidates = [nltk.word_tokenize(decode_sequence(text.reshape(1,max_text_len))) for text in x_val[:evaluation_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NQZggx7cXaE",
    "outputId": "4d25d104-67ed-4059-889f-5706c1912750",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.025008283553595e-79\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    bleu_score = corpus_bleu(tokenized_references, tokenized_candidates)\n",
    "    print(bleu_score)\n",
    "except:\n",
    "    print(\"ERROR\")\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "    bleu_score = corpus_bleu(tokenized_references, tokenized_candidates, smoothing_function=SmoothingFunction().method1)\n",
    "    print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_summary = [x[0] for x in tokenized_references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_text = [nltk.word_tokenize(seq2text(x)) for x in x_val[:evaluation_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load the English model for spaCy\n",
    "nlp = spacy.load('en_core_web_md')  # Load the spaCy model for English language\n",
    "\n",
    "# Define a function to compare cosine similarity between two texts\n",
    "def compare_cosine_similarity(text1, text2, debug=False):\n",
    "    # Process the texts using spaCy to tokenize and assign word vectors\n",
    "    doc1 = nlp(text1)  # Tokenization and word vectorization for text1\n",
    "    doc2 = nlp(text2)  # Tokenization and word vectorization for text2\n",
    "    \n",
    "    if debug:\n",
    "        # print(len(text1.split())) # 41\n",
    "        # print(type(doc1))\n",
    "        # print(doc1)\n",
    "        # print([token.vector for token in doc1 if token.has_vector])\n",
    "        # print(len([token.vector for token in doc1 if token.has_vector])) # 41\n",
    "        # print(len([token.vector for token in doc1 if token.has_vector][0])) # 300\n",
    "        pass\n",
    "    \n",
    "    # Calculate the average word vectors for each document\n",
    "    vec1 = np.mean([token.vector for token in doc1 if token.has_vector], axis=0)  # Vector averaging for text1 (each embedding has 300 values)\n",
    "    vec2 = np.mean([token.vector for token in doc2 if token.has_vector], axis=0)  # Vector averaging for text2 (each embedding has 300 values)\n",
    "    \n",
    "    # if debug:\n",
    "    #     print(vec1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the vectors\n",
    "    similarity_score = cosine_similarity([vec1], [vec2])[0][0]  # Cosine similarity calculation between vec1 and vec2\n",
    "    return similarity_score  # Return the cosine similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.0%\n",
      "300\n",
      "progress: 20.0%\n",
      "progress: 40.0%\n",
      "progress: 60.0%\n",
      "progress: 80.0%\n"
     ]
    }
   ],
   "source": [
    "similarity = []  # Initialize a list to store similarity scores\n",
    "similarity_summary = []\n",
    "\n",
    "for i in range(evaluation_number):\n",
    "    if i%100 == 0:\n",
    "        print(f'progress: {(i*100)/evaluation_number}%')  # tracking progress\n",
    "    debug = False if i > 0 else True\n",
    "    similarity.append(compare_cosine_similarity(' '.join(original_text[i]), ' '.join(tokenized_candidates[i]), debug))  # Compute and store the cosine similarity between the original text and its summary\n",
    "    similarity_summary.append(compare_cosine_similarity(' '.join(actual_summary[i]), ' '.join(tokenized_candidates[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine simmilarity with the text is 0.6266710266217124\n",
      "The cosine simmilarity with the summary is 0.5038295055746567\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average similarity score\n",
    "average1 = sum(similarity) / len(similarity)  \n",
    "average2 = sum(similarity_summary) / len(similarity_summary)  \n",
    "\n",
    "print(f'The cosine simmilarity with the text is {average1}')\n",
    "print(f'The cosine simmilarity with the summary is {average2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTkaYNjHW4lC"
   },
   "source": [
    "Finally, Our model is able to generate a meaningful summary based on the context present in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zob_sgYe7d59"
   },
   "source": [
    "## Text Summarization using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T10:37:28.199624Z",
     "iopub.status.busy": "2021-07-18T10:37:28.199290Z",
     "iopub.status.idle": "2021-07-18T10:37:28.205986Z",
     "shell.execute_reply": "2021-07-18T10:37:28.204830Z",
     "shell.execute_reply.started": "2021-07-18T10:37:28.199592Z"
    },
    "id": "tIdXfKXd7d59"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcq5HfE97d59"
   },
   "source": [
    "Summarize news articles and other documents.\n",
    "This summarizing pipeline can currently be loaded from pipeline() using the following task identifier: \"summarization\".\n",
    "The models that this pipeline can use are models that have been fine-tuned on a summarization task, which is currently, ‘bart-large-cnn’, ‘t5-small’, ‘t5-base’, ‘t5-large’, ‘t5-3b’, ‘t5-11b’.\n",
    "\n",
    "https://huggingface.co/t5-base\n",
    "\n",
    "https://huggingface.co/models?filter=summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:15:30.034938Z",
     "iopub.status.busy": "2021-07-18T16:15:30.034498Z",
     "iopub.status.idle": "2021-07-18T16:15:30.268895Z",
     "shell.execute_reply": "2021-07-18T16:15:30.267944Z",
     "shell.execute_reply.started": "2021-07-18T16:15:30.034851Z"
    },
    "id": "sePsOBZu7d59"
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "execution": {
     "iopub.execute_input": "2021-07-18T16:15:37.634640Z",
     "iopub.status.busy": "2021-07-18T16:15:37.634248Z",
     "iopub.status.idle": "2021-07-18T16:15:38.807490Z",
     "shell.execute_reply": "2021-07-18T16:15:38.806628Z",
     "shell.execute_reply.started": "2021-07-18T16:15:37.634580Z"
    },
    "id": "_RQsl1347d59",
    "outputId": "de4cadbf-bfa5-4307-e825-80e14778c253"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-11d591aebcf7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscraped_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://en.wikipedia.org/wiki/Artificial_intelligence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraped_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparsed_article\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_article\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxTargetStartNoNs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._callTargetSaxStart\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_NSMAPS_INVERTED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Make sure attrs is a mutable dict--lxml may send an immutable dictproxy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scraped_data.read()\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs = parsed_article.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:15:43.123473Z",
     "iopub.status.busy": "2021-07-18T16:15:43.123111Z",
     "iopub.status.idle": "2021-07-18T16:15:43.130507Z",
     "shell.execute_reply": "2021-07-18T16:15:43.129565Z",
     "shell.execute_reply.started": "2021-07-18T16:15:43.123438Z"
    },
    "id": "NR7P0U1n7d59"
   },
   "outputs": [],
   "source": [
    "article_text = \"\"\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:16:29.081781Z",
     "iopub.status.busy": "2021-07-18T16:16:29.081424Z",
     "iopub.status.idle": "2021-07-18T16:16:29.089070Z",
     "shell.execute_reply": "2021-07-18T16:16:29.088151Z",
     "shell.execute_reply.started": "2021-07-18T16:16:29.081749Z"
    },
    "id": "_NkUUA-a7d59"
   },
   "outputs": [],
   "source": [
    "article_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:16:33.342864Z",
     "iopub.status.busy": "2021-07-18T16:16:33.342496Z",
     "iopub.status.idle": "2021-07-18T16:16:33.351787Z",
     "shell.execute_reply": "2021-07-18T16:16:33.350510Z",
     "shell.execute_reply.started": "2021-07-18T16:16:33.342822Z"
    },
    "id": "03exbqfG7d5-"
   },
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "\n",
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:16:39.993030Z",
     "iopub.status.busy": "2021-07-18T16:16:39.992717Z",
     "iopub.status.idle": "2021-07-18T16:16:40.004144Z",
     "shell.execute_reply": "2021-07-18T16:16:40.003354Z",
     "shell.execute_reply.started": "2021-07-18T16:16:39.993002Z"
    },
    "id": "xCI3c70j7d5-"
   },
   "outputs": [],
   "source": [
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:16:47.019757Z",
     "iopub.status.busy": "2021-07-18T16:16:47.019310Z",
     "iopub.status.idle": "2021-07-18T16:16:47.025327Z",
     "shell.execute_reply": "2021-07-18T16:16:47.024339Z",
     "shell.execute_reply.started": "2021-07-18T16:16:47.019711Z"
    },
    "id": "Oa-4vseA7d5-"
   },
   "outputs": [],
   "source": [
    "len(formatted_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:16:54.298439Z",
     "iopub.status.busy": "2021-07-18T16:16:54.298128Z",
     "iopub.status.idle": "2021-07-18T16:16:54.302336Z",
     "shell.execute_reply": "2021-07-18T16:16:54.301337Z",
     "shell.execute_reply.started": "2021-07-18T16:16:54.298411Z"
    },
    "id": "SmSxaD2h7d5_"
   },
   "outputs": [],
   "source": [
    "formatted_article_text_1=formatted_article_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-07-18T16:19:29.552693Z",
     "iopub.status.busy": "2021-07-18T16:19:29.552320Z",
     "iopub.status.idle": "2021-07-18T16:20:06.155179Z",
     "shell.execute_reply": "2021-07-18T16:20:06.154236Z",
     "shell.execute_reply.started": "2021-07-18T16:19:29.552653Z"
    },
    "id": "L3v9NxiZ7d5_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#use t5 in tf\n",
    "summarizer1 = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:21:02.002098Z",
     "iopub.status.busy": "2021-07-18T16:21:02.001497Z",
     "iopub.status.idle": "2021-07-18T16:21:02.008225Z",
     "shell.execute_reply": "2021-07-18T16:21:02.007446Z",
     "shell.execute_reply.started": "2021-07-18T16:21:02.002057Z"
    },
    "id": "Pgg1L7I87d5_"
   },
   "outputs": [],
   "source": [
    "formatted_article_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:20:25.371110Z",
     "iopub.status.busy": "2021-07-18T16:20:25.370740Z",
     "iopub.status.idle": "2021-07-18T16:20:47.799100Z",
     "shell.execute_reply": "2021-07-18T16:20:47.798307Z",
     "shell.execute_reply.started": "2021-07-18T16:20:25.371079Z"
    },
    "id": "FOUcsRiz7d5_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarizer1(formatted_article_text_1, min_length=5, max_length=500,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:20:51.567139Z",
     "iopub.status.busy": "2021-07-18T16:20:51.566812Z",
     "iopub.status.idle": "2021-07-18T16:21:01.999878Z",
     "shell.execute_reply": "2021-07-18T16:21:01.998786Z",
     "shell.execute_reply.started": "2021-07-18T16:20:51.567110Z"
    },
    "id": "LEVO0-NF7d5_"
   },
   "outputs": [],
   "source": [
    "summarizer1(formatted_article_text_1, min_length=5, max_length=20,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:24:20.965777Z",
     "iopub.status.busy": "2021-07-18T16:24:20.965408Z",
     "iopub.status.idle": "2021-07-18T16:24:20.974953Z",
     "shell.execute_reply": "2021-07-18T16:24:20.973846Z",
     "shell.execute_reply.started": "2021-07-18T16:24:20.965742Z"
    },
    "id": "FqmXcbOj7d5_"
   },
   "outputs": [],
   "source": [
    "text_string=data.Text[0]\n",
    "text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T16:25:12.902082Z",
     "iopub.status.busy": "2021-07-18T16:25:12.901756Z",
     "iopub.status.idle": "2021-07-18T16:25:23.456627Z",
     "shell.execute_reply": "2021-07-18T16:25:23.455823Z",
     "shell.execute_reply.started": "2021-07-18T16:25:12.902055Z"
    },
    "id": "HC67vaWo7d5_"
   },
   "outputs": [],
   "source": [
    "summarizer1(text_string, min_length=5, max_length=20,do_sample=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "hide_input": false,
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.11.0 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/tensorflow-2.11.0-gpu-py39-cu112-ubuntu20.04-sagemaker-v1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
